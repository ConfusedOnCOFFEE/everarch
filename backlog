* combine ref and claim index into cref
most likely this will be 28 bytes of key + 2 bytes of unsigned int
index.

change the types in the attr-index-db. add formatter and parser for
cref.
* define query language and result format for evr-attr-index server
expression examples:

#+BEGIN_SRC
tag=peng color=blue size<=12 is_image(mime_type) -key -key2=peng
-tag=nsfw
(tag=todo || tag=backlog) tag=howto
#+END_SRC

implement an ast and parser for the query language.

define a format for results the query can find.
** TODO how can we search for a single blob
something like "ref:sha3-224-â€¦" but without the issue that "ref:"
would shadow a ref attribute defined in a claim.
** DONE how can we defined what kind of results we want?
maybe only a limited set of attributes is required? maybe some
aggregation functions should be invoked?

-> a minimalistic query will always just return refs of matching
   claims. it will probably make sense to specify a set of attribute
   keys which should also be responded.
** DONE how can we map the query to select statements?
#+BEGIN_SRC sql
;; tag=peng
select cref from claim where
  cref in (select cref from attr where key = ? and val_str = ?)
;; [(txt, 'tag'), (txt, 'peng')]

;; tag=a && tag=b
select cref from claim where
  (cref in (select cref from attr where key = ? and val_str = ?)
   and cref in (select cref from attr where key = ? and val_str = ?))
;; [(txt, 'tag'), (txt, 'a'), (txt, 'tag'), (txt, 'b')]
#+END_SRC
* add index populate thread to evr-attr-index
should wait for index claims from index claim watch thread.

then build a sqlite index db based on index claim. watch a glacier for
claims and populate the sqlite db until the batch ends.

the recent sqlite db should be provided to consumers.
* evr-attr-index should listen on a socket for querries
* introduce evr-attr-cli cmd with index query feature
command line client which sends querries to a evr-attr-index and lists
found blobs.
* introduce content sourced attritutes
attr specs should contain declarations for content sourced
attributes. content sourced attributes are scripts which are executed
with every claim and can produce an attr claim as output.

the attr spec for content based attributes might look like this:

#+BEGIN_SRC xml
<attr-spec>
  <attr-def k="tag" type="str"/>
  <stylesheet blob="sha3-224-00000000000000000000000000000000000000000000000000000000"/>
  <attr-factory blob="sha3-224-00000000000000000000000000000000000000000000000000000001"/>
</attr-spec>
#+END_SRC

the attr-factory must point to an executable. for example an elf
binary, shell or python script.
* store flags and blob size redundant in buckets
right now flags and blob size in the evr-glacier-storage are storend
once for every blob and without any checksum.

there should be either a checksum for the two fields or it should be
stored redundant. this should reduce the risk of breaking a whole
bucket if one blob in it has a bit error.
* move glacier storage client functions into glacier-client.[hc]
right now the evr-glacier-cli.c contains functions for handling blob
put and blob get commands. code should be centralized in
glacier-client.[ch] in order to be reused in other future clients like
the file upload service.
* add setup hints to README
** gpg signing key
hint the user that the default gpg signing key is used for signing
claims. don't lose it and be aware of etc.
** config file locations
indicate where config files should be placed and how they should look
* replace struct chunk_set with linked list
dny-mem.h should provide one growable random access data structure and
one growable linked list data structure.

especially struct dynamic_array is not very well fitted for growing
because of the evr_chunk_set_max_chunks limitation.

struct dynamic_array should be renamed to hint the random access
usage. maybe rabuf instead of dynamic_array and llbuf for chunk_set.
* replace own queues with lib io_uring
inter thread communitcation queues have been implemented in struct
evr_work_watch_ctx and struct evr_persister_ctx.

for the sake of using well tested code we should check migrating our
own queues to lib io_uring. mentioned by pitrp.
* implement an evr-glacier-storage blob put/get benchmark
should connect via network socket. send lots of blobs. request lots of
blobs. count blobs per second. maybe different blob sizes etc.
* add option for flags to evr-glacier-cli put command
right now flags=0 is always sent when putting a blob to the storage.

the put flags should be specifyable using command line arguments in
the evr-glacier-cli tool.
* align chunk size with RLIMIT_FSIZE
see [[elisp:(manual-entry "setrlimit(2)")][setrlimit(2)]] for retrieving RLIMIT_FSIZE and limit the chunk size
regarding to it. also the chunk size should not grow over 1MB.
* create bucket directory if not existing :bdircre:
* repair out of sync bucket end pointer and file end offset :beprep:
the whole bucket should be scanned with validation of each key and
blob. if the file probably ends with one corrupt blob it should be
discarded.

it's important to be sensible about the discarded data at the end of
the file. maybe there should be an 'autorepair' flag in the
configuration.
